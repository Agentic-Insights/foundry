# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "// LLM Provider Configuration\n// Local testing: Ollama | Production: Bedrock/Anthropic\n\n// ============================================\n// LOCAL TESTING (Ollama on Windows host)\n// ============================================\n\n// Primary local model - Qwen3 VL (good JSON compliance)\nclient<llm> OllamaQwen {\n  provider \"openai-generic\"\n  options {\n    base_url \"http://172.30.224.1:11434/v1\"\n    model \"qwen3-vl:30b\"\n    api_key \"ollama\"\n  }\n}\n\n// Alternative: GPT-OSS (latest - better JSON compliance)\nclient<llm> OllamaGPT {\n  provider \"openai-generic\"\n  options {\n    base_url \"http://172.30.224.1:11434/v1\"\n    model \"gpt-oss:latest\"\n    api_key \"ollama\"\n  }\n}\n\n// Fallback chain for local testing\nclient<llm> LocalExtractor {\n  provider fallback\n  options {\n    strategy [OllamaQwen, OllamaGPT]\n  }\n}\n\n// ============================================\n// PRODUCTION (Bedrock/Anthropic)\n// ============================================\n\n// Anthropic Claude Haiku (fast + cheap)\nclient<llm> ClaudeHaiku {\n  provider anthropic\n  options {\n    api_key env.ANTHROPIC_API_KEY\n    model \"claude-haiku-4-20250514\"\n    max_tokens 1024\n  }\n}\n\n// Anthropic Claude Sonnet (complex extraction)\nclient<llm> ClaudeSonnet {\n  provider anthropic\n  options {\n    api_key env.ANTHROPIC_API_KEY\n    model \"claude-sonnet-4-20250514\"\n    max_tokens 2048\n  }\n}\n\n// Production fallback with Anthropic\nclient<llm> ProductionExtractor {\n  provider fallback\n  options {\n    strategy [ClaudeHaiku, ClaudeSonnet]\n  }\n}\n\n// ============================================\n// ACTIVE CLIENT (change for deployment)\n// ============================================\n\n// For local testing, use Ollama\n// For production, switch to ProductionExtractor or ClaudeHaiku\nclient<llm> Extractor {\n  provider fallback\n  options {\n    strategy [OllamaQwen, OllamaGPT]\n  }\n}\n",
    "generators.baml": "// BAML Code Generator Configuration\n// Generates type-safe Python client for memory event extraction\n\ngenerator target {\n  output_type python/pydantic\n  output_dir \"../baml_client\"\n  version \"0.214.0\"\n}\n",
    "memory_extraction.baml": "// Memory Event Extraction for AWS Bedrock AgentCore\n// Type-safe extraction of conversation messages from memory events\n\n// ============================================\n// Schema: AgentCore Memory Event Structure\n// ============================================\n\n// Role enum with strict validation\nenum MessageRole {\n  USER\n  ASSISTANT\n  SYSTEM\n}\n\n// Extracted conversation message (output format)\nclass ConversationMessage {\n  role MessageRole @description(\"The role of the message sender\")\n  content string @description(\"The text content of the message\")\n}\n\n// Agent response with optional structured metadata\nclass AgentResponse {\n  main_response string @description(\"The primary response content from the agent\")\n  tools_used string[]? @description(\"List of tools that were invoked during this turn\")\n  confidence float? @description(\"Confidence score from 0.0 to 1.0\")\n  requires_followup bool @description(\"Whether the agent suggests a follow-up question\")\n}\n\n// Complete conversation with metadata\nclass Conversation {\n  messages ConversationMessage[] @description(\"Ordered list of messages in the conversation\")\n  turn_count int @description(\"Number of conversation turns (non-negative)\")\n  has_tool_calls bool @description(\"Whether any tools were called in this conversation\")\n}\n\n// ============================================\n// Extraction Functions\n// ============================================\n\n// Extract conversation messages from raw AgentCore memory events\n// This replaces the error-prone manual parsing in agent.py lines 88-112\nfunction ExtractConversation(raw_events: string) -> Conversation {\n  client Extractor\n  prompt #\"\n    You are a precise JSON parser. Extract conversation messages from AWS Bedrock AgentCore memory events.\n\n    The input is raw memory event data with nested structure:\n    - events is a list of event objects\n    - each event has a payload list\n    - each payload item may have a \"conversational\" field\n    - conversational contains role (USER/ASSISTANT) and content.text\n\n    Extract ALL messages in chronological order.\n\n    Raw events data:\n    {{ raw_events }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Extract and enhance an agent response with metadata\nfunction ExtractAgentResponse(raw_response: string, tool_names: string) -> AgentResponse {\n  client Extractor\n  prompt #\"\n    Analyze this agent response and extract structured information.\n\n    Agent response:\n    {{ raw_response }}\n\n    Available tools that may have been used: {{ tool_names }}\n\n    Determine:\n    1. The main response content\n    2. Which tools (if any) were used (based on response content patterns)\n    3. Estimate confidence (high confidence = definitive answers, low = uncertain/hedged language)\n    4. Whether a follow-up question is warranted\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Validate and normalize a single message\nfunction ValidateMessage(role: string, content: string) -> ConversationMessage {\n  client Extractor\n  prompt #\"\n    Normalize this message into the correct format.\n\n    Input role (may be lowercase or uppercase): {{ role }}\n    Input content: {{ content }}\n\n    Rules:\n    - Role must be exactly USER, ASSISTANT, or SYSTEM\n    - Content should be trimmed of leading/trailing whitespace\n    - If role is unrecognized, default to USER\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Quick sentiment check on agent response (useful for monitoring)\nfunction AnalyzeResponseTone(response: string) -> ResponseAnalysis {\n  client Extractor\n  prompt #\"\n    Briefly analyze the tone and helpfulness of this agent response.\n\n    Response: {{ response }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nclass ResponseAnalysis {\n  tone string @description(\"Brief description: helpful, confused, apologetic, etc.\")\n  is_error_message bool @description(\"Whether this appears to be an error or failure message\")\n  suggested_improvement string? @description(\"Optional suggestion for improving the response\")\n}\n",
    "tests.baml": "// BAML Tests for Memory Extraction Functions\n// Run with: baml-cli test\n\n// Test extracting a simple conversation from memory events\ntest ExtractSimpleConversation {\n  functions [ExtractConversation]\n  args {\n    raw_events #\"\n      [\n        {\n          \"event_id\": \"evt-001\",\n          \"payload\": [\n            {\n              \"conversational\": {\n                \"role\": \"USER\",\n                \"content\": {\"text\": \"What is the weather in Seattle?\"}\n              }\n            }\n          ]\n        },\n        {\n          \"event_id\": \"evt-002\",\n          \"payload\": [\n            {\n              \"conversational\": {\n                \"role\": \"ASSISTANT\",\n                \"content\": {\"text\": \"I searched for weather information. Seattle is currently 55F and cloudy.\"}\n              }\n            }\n          ]\n        }\n      ]\n    \"#\n  }\n  @@assert({{ this.messages | length == 2 }})\n  @@assert({{ this.messages[0].role == \"USER\" }})\n  @@assert({{ this.messages[1].role == \"ASSISTANT\" }})\n  @@assert({{ this.turn_count >= 1 }})\n}\n\n// Test with multiple turns\ntest ExtractMultiTurnConversation {\n  functions [ExtractConversation]\n  args {\n    raw_events #\"\n      [\n        {\"event_id\": \"1\", \"payload\": [{\"conversational\": {\"role\": \"USER\", \"content\": {\"text\": \"Hello\"}}}]},\n        {\"event_id\": \"2\", \"payload\": [{\"conversational\": {\"role\": \"ASSISTANT\", \"content\": {\"text\": \"Hi there! How can I help?\"}}}]},\n        {\"event_id\": \"3\", \"payload\": [{\"conversational\": {\"role\": \"USER\", \"content\": {\"text\": \"Search for Python tutorials\"}}}]},\n        {\"event_id\": \"4\", \"payload\": [{\"conversational\": {\"role\": \"ASSISTANT\", \"content\": {\"text\": \"I found several Python tutorials on the web.\"}}}]}\n      ]\n    \"#\n  }\n  @@assert({{ this.messages | length == 4 }})\n  @@assert({{ this.turn_count >= 2 }})\n}\n\n// Test extracting agent response with tool usage\ntest ExtractResponseWithTools {\n  functions [ExtractAgentResponse]\n  args {\n    raw_response \"Based on my search using DuckDuckGo, I found that Python 3.12 was released in October 2023. The search returned several results from python.org and various programming blogs.\"\n    tool_names \"DuckDuckGo, Calculator\"\n  }\n  @@assert({{ \"DuckDuckGo\" in this.tools_used }})\n  @@assert({{ this.requires_followup == false }})\n}\n\n// Test response without tools\ntest ExtractSimpleResponse {\n  functions [ExtractAgentResponse]\n  args {\n    raw_response \"I'm not sure about that. Could you provide more context about what you're looking for?\"\n    tool_names \"DuckDuckGo, Calculator\"\n  }\n  @@assert({{ not this.tools_used or this.tools_used | length == 0 }})\n  @@assert({{ this.requires_followup == true }})\n}\n\n// Test message validation and normalization\ntest ValidateLowercaseRole {\n  functions [ValidateMessage]\n  args {\n    role \"user\"\n    content \"  Hello world  \"\n  }\n  @@assert({{ this.role == \"USER\" }})\n}\n\n// Test response tone analysis\ntest AnalyzeHelpfulResponse {\n  functions [AnalyzeResponseTone]\n  args {\n    response \"I found the information you were looking for! Here are the top 5 results with detailed explanations.\"\n  }\n  @@assert({{ this.is_error_message == false }})\n}\n\ntest AnalyzeErrorResponse {\n  functions [AnalyzeResponseTone]\n  args {\n    response \"I encountered an error while processing your request. The service is temporarily unavailable.\"\n  }\n  @@assert({{ this.is_error_message == true }})\n}\n\n// Test edge case: empty events\ntest ExtractEmptyConversation {\n  functions [ExtractConversation]\n  args {\n    raw_events \"[]\"\n  }\n  @@assert({{ this.messages | length == 0 }})\n  @@assert({{ this.turn_count == 0 }})\n}\n",
}

def get_baml_files():
    return _file_map